#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass paper
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine natbib_authoryear
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
CS224N Programming Assignment 4 Report
\end_layout

\begin_layout Author
Daniel Chia (danchia@stanford.edu), Frank Chen (frankchn@stanford.edu)
\end_layout

\begin_layout Standard
We used 
\emph on
three
\emph default
 late days for this assignment.
 This assignment is submitted both electronically and in paper form on 
\emph on
December 8th, 2012 at 11am
\emph default
.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In Programming Assignment 4, we are asked to implement a neural network
 for named entity recognition.
 We provide derivations, descriptions of our methods, implementating details,
 test results and discussion about the pros and cons of our implementation.
\end_layout

\begin_layout Standard
Frank wrote the code to load in and preprocess the matrices given while
 Daniel derived the equations and wrote the code for forward and backpropagation.
 Frank worked on word vector visualization and the derivation of the equivalence
 of the softmax classifier and the logistic regression classifier.
 Frank and Daniel analyzed the results and wrote this report together.
\end_layout

\begin_layout Section
Feedforward and Cost Functions
\end_layout

\begin_layout Standard
As in the notes, we denote the forward propagation stage, by the following
 equations, eventually resulting in a prediction 
\begin_inset Formula $h_{\theta}(x)$
\end_inset

 of whether the word is a PERSON.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
z & = & Wx+b_{1}\\
a & = & f(z)\qquad\mathrm{where}\; f(x)=\tanh(x)\\
h & = & g(U^{T}a+b_{2})\qquad\mathrm{where}\: g(x)=\frac{1}{1+e^{-x}}\\
J(\theta) & = & \frac{1}{m}\sum\left[-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i0})h_{\theta}(x^{(i)})\right]\:+\;\frac{C}{2m}\left[\sum_{j=1}^{nC}\sum_{k=1}^{H}W_{kj}^{2}+\sum_{k=1}^{H}U_{k}^{2}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Gradients
\end_layout

\begin_layout Standard
Here, we present the derived gradients used to train the model.
 Full derivations can be found in the appendix.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial J(\theta)}{\partial U} & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}+(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}\right)h_{\theta}(x^{(i)})(1-h_{\theta}(x^{(i)}))\frac{\partial(U^{T}a^{(i)}+b_{2})}{\partial U}\right]+\frac{C}{m}U\\
 &  & \mathrm{since\:}g'(x)=g(x)(1-g(x))\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\, a^{(i)}\right]+\frac{C}{m}U
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial J(\theta)}{\partial b_{2}} & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}+(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}\right)h_{\theta}(x^{(i)})(1-h_{\theta}(x^{(i)}))\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial J(\theta)}{\partial W_{kj}} & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\frac{\partial(U^{T}a^{(i)}+b_{2})}{\partial W_{kj}}\right]+\frac{C}{2m}(2W_{kj})\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}f'(z_{k}^{(i)})\frac{\partial(W_{k\cdot}x^{(i)}+(b_{1})_{k})}{\partial W_{kj}}\right]+\frac{C}{m}W_{kj}\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}(1-\tanh^{2}z_{k}^{(i)})x_{j}^{(i)}\right]+\frac{C}{m}W_{kj}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\mathrm{Let}\:\delta_{k}^{(i)} & = & \left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}(1-\tanh^{2}z_{k}^{(i)})\\
\frac{\partial J(\theta)}{\partial W} & = & \frac{1}{m}\sum_{i=1}^{m}\delta^{(i)}\left(x^{(i)}\right)^{T}+\frac{C}{m}W\frac{\partial J(\theta)}{\partial(b_{1})_{i}}\\
 &  & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}f'(z_{k}^{(i)})\frac{\partial(W_{k\cdot}x^{(i)}+(b_{1})_{k})}{\partial W_{kj}}\right]\\
 &  & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}f'(z_{k}^{(i)})\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial J(\theta)}{\partial b_{1}} & = & \frac{1}{m}\sum_{i=1}^{m}\delta^{(i)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial J(\theta)}{\partial L_{k}} & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\frac{\partial(U^{T}a^{(i)}+b_{2})}{\partial L_{k}}\right]\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\left(\sum_{j=1}^{H}U_{j}f'(z_{j}^{(i)})\frac{\partial(W_{j\cdot}x^{(i)}+(b_{1})_{j})}{\partial L_{k}}\right)\right]\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\left(\sum_{j=1}^{H}U_{j}f'(z_{j}^{(i)})W_{jk}\right)\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial J(\theta)}{\partial L} & = & \frac{1}{m}\sum_{i=1}^{m}W^{T}\delta^{(i)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
SGD Training and Implementation
\end_layout

\begin_layout Standard
We translated and implemented the gradients and SGD in a straight-forward
 manner in Java.
 Please see our source code for more details.
\end_layout

\begin_layout Subsection
Results
\end_layout

\begin_layout Standard
The results as evaluated using the 
\begin_inset Formula $\texttt{conlleval.pl}$
\end_inset

 on the 
\begin_inset Formula $\texttt{dev}$
\end_inset

 dataset is as follows:
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Tag
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Precision
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recall
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
PERSON
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
56.20%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
61.84%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
58.89%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Section
Network Analysis
\end_layout

\begin_layout Subsection
Hyperparameter Variation
\end_layout

\begin_layout Standard
Note: Results below show token-based F1 scores.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename tune_f1_0.0002.png
	width 2.25in

\end_inset


\begin_inset Graphics
	filename tune_f1_0.001.png
	width 2.25in

\end_inset


\begin_inset Graphics
	filename tune_f1_0.005.png
	width 2.25in

\end_inset


\end_layout

\begin_layout Standard
We have used token-based development F1-scores to tune our hyperparameters
 rather than entity-level evaluation and we varied the window size from
 3 to 7 in steps of 2, number of neurons in the hidden layer between 50
 and 200, and the learning rate 
\begin_inset Formula $\alpha$
\end_inset

 between 0.0002 and 0.005.
 We have plotted the results of the graphs as above.
 All our training is done with 
\begin_inset Formula $C=20$
\end_inset

 (i.e.
 20 iterations).
\end_layout

\begin_layout Standard
From our graph, we can see that performance on the dev-test decreases when
 the window size is increased beyond 5 uness either 
\begin_inset Formula $\alpha=0.0002$
\end_inset

 or the size of the hidden layer is 50.
 We observed a similar trend in the F1 scores on our training set as well.
 
\end_layout

\begin_layout Standard
We believe this may be due to overfitting to the training set.
 As our data size is rather small, the increase in our learning rates and
 the number of hidden units will cause our data to conform far too closely
 to the training set and thus perform rather poorly on the test set.
 
\end_layout

\begin_layout Subsection
Error Analysis
\end_layout

\begin_layout Subsubsection
Misclassification of 
\begin_inset Formula $\texttt{O}$
\end_inset

 as 
\begin_inset Formula $\texttt{PERSON}$
\end_inset


\end_layout

\begin_layout Paragraph
Place Names and other Proper Nouns
\end_layout

\begin_layout Standard
There were significant instances of misclassification of proper nouns, such
 as 
\begin_inset Quotes eld
\end_inset

National Observatory
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Korea
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Mercedes-Benz
\begin_inset Quotes erd
\end_inset

.
 These words and phrases often have the same characteristics (including
 the frequency of appearance and capitalization) and appear in the same
 contexts as phrases refering to individuals.
 This will throw our algorithms off.
\end_layout

\begin_layout Paragraph
Dates and Numbers
\end_layout

\begin_layout Standard
We are also classifying dates and numbers (e.g.
 1996-08-31, 3, 2) as person entities.
 We could easily remedy this by forcing the classification of any entity
 with numbers in them as 
\begin_inset Formula $\texttt{O}$
\end_inset

 instead of 
\begin_inset Formula $\texttt{PERSON}$
\end_inset

.
\end_layout

\begin_layout Paragraph
Pronouns
\end_layout

\begin_layout Standard
Finally, classification of 
\begin_inset Quotes eld
\end_inset

I
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

He
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

She
\begin_inset Quotes erd
\end_inset

 as persons are also common.
 We believe that as these pronouns often appear in the same location as
 people names, the algorithm would be confused.
 An easy fix for this would merely be adding the list of proper nouns and
 forcing the classification of any proper noun detected as 
\begin_inset Quotes eld
\end_inset

O
\begin_inset Quotes erd
\end_inset

 rather than 
\begin_inset Quotes eld
\end_inset

PERSON.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsubsection
Misclassification of 
\begin_inset Formula $\texttt{PERSON}$
\end_inset

 as 
\begin_inset Formula $\texttt{O}$
\end_inset


\end_layout

\begin_layout Paragraph
Non-English Names
\end_layout

\begin_layout Standard
Our system often misclassifies non-English first and last names as 
\begin_inset Formula $\texttt{O}$
\end_inset

 rather than 
\begin_inset Formula $\texttt{PERSON}$
\end_inset

.
 We believe this is because we have not seen the word in the wordVector
 or the training data before, and we have to use the UNK token as a substitute
 for the word itself and only context to figure out whether the current
 word is a 
\begin_inset Formula $\texttt{PERSON}$
\end_inset

 reference or not.
\end_layout

\begin_layout Paragraph
Usage of Only Last Names
\end_layout

\begin_layout Standard
In some instances (e.g.
 ...in Gulf await 
\emph on
Clinton
\emph default
 order..., 
\emph on
Wang
\emph default
 was jailed for....), our neural network did not manage to identify the single
 token as a person entity.
 We hypothesize this is due to the context of the surrounding tokens.
 These words may only have activations which cross the threshold when certain
 other words (e.g.
 other similar names -- such as Bill Clinton) are in the same context as
 it.
\end_layout

\begin_layout Paragraph
Shorted Names (e.g.
 P., J.)
\end_layout

\begin_layout Standard
Shortened names such as P.
 and J.
 is not correctly classified as part of person.
 We believe that this is because the tokens that form these shortened names
 (e.g.
 
\begin_inset Quotes eld
\end_inset

P.
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

W.
\begin_inset Quotes erd
\end_inset

, etc...) are also part of acronyms in our training text that are not referring
 to persons.
 This overloaded usage of the token may well throw our simple classifier
 off, especially when the training data does not contain any shortened names
 with that specific token.
\end_layout

\begin_layout Subsubsection
Classifying only parts of Entities
\end_layout

\begin_layout Paragraph
Classification of Titles
\end_layout

\begin_layout Standard
The algorithm believed that the words 
\begin_inset Quotes eld
\end_inset

Cardinal
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

General
\begin_inset Quotes erd
\end_inset

 in 
\begin_inset Quotes eld
\end_inset

Cardinal Wolsey
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

General Kutuzov
\begin_inset Quotes erd
\end_inset

 are part of the named entity as well.
 We believe that the correctness of our results versus the gold solution
 is debatable.
 We believe that marking 
\begin_inset Quotes eld
\end_inset

Cardinal Wolsey
\begin_inset Quotes erd
\end_inset

 as an entity rather than just 
\begin_inset Quotes eld
\end_inset

Wolsey
\begin_inset Quotes erd
\end_inset

 is equally correct since 
\begin_inset Quotes eld
\end_inset

Cardinal Wolsey
\begin_inset Quotes erd
\end_inset

 refers to a person.
\end_layout

\begin_layout Subsection
Word Vector Visualization
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename vector_before.png
	lyxscale 20
	scale 10

\end_inset


\begin_inset Graphics
	filename vector_after.png
	lyxscale 20
	scale 10

\end_inset


\end_layout

\begin_layout Standard
We have also visualized a random sampling of L vectors before and after
 training using the t-SNE algorithm, as suggested by the assignment handout.
 
\end_layout

\begin_layout Standard
The L vectors before and after training does not seem to have much difference
 at first glance.
 However, we note that the center area is smaller in the L vector after
 training, and there are more pronounced clusters in the L-vectors after
 training than before.
 The center cluster of individual vectors were also smaller than before
 training.
 We believe additional clustering is observed because those words often
 appear in the context window together in our training set and thus would
 be grouped closer to each other after our training is done than the starter
 L vectors given in the starter code.
\end_layout

\begin_layout Standard
In addition, we also observed about 90% of L vectors which were unchanged
 during training as those words did not exist in the training set and thus
 training on our current dataset would not be useful.
 We believe that a larger dataset would definitely be more useful for more
 coverage of all the L vectors and improve the performance of our neural
 network.
\end_layout

\begin_layout Subsection
Softmax Classifier Equivalence to Logistic Regression Classifier
\end_layout

\begin_layout Standard
Given a softmax classifier with 
\begin_inset Formula $k=2$
\end_inset

, the classifier will output the following
\begin_inset Foot
status open

\begin_layout Plain Layout
CS229 Lecture Notes 1 Page 26
\end_layout

\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
h_{\theta}(x) & = & E[T(y)|x;\theta]\\
 & = & \left[\begin{array}{c}
\phi_{1}\\
\phi_{2}
\end{array}\right]\\
 & = & \left[\begin{array}{c}
\frac{\exp(\theta_{1}^{T}x)}{\exp(\theta_{1}^{T}x)+\exp(\theta_{2}^{T}x)}\\
\frac{\exp(\theta_{2}^{T}x)}{\exp(\theta_{1}^{T}x)+\exp(\theta_{2}^{T}x)}
\end{array}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We note that one of 
\begin_inset Formula $\theta_{1}$
\end_inset

 and 
\begin_inset Formula $\theta_{2}$
\end_inset

 is redundant and that we can subtract 
\begin_inset Formula $\theta_{2}$
\end_inset

 from all the parameters, and letting 
\begin_inset Formula $\theta'=\theta_{1}-\theta_{2}$
\end_inset

 (since 
\begin_inset Formula $\theta_{2}-\theta_{2}=0$
\end_inset

), we thus have:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
h_{\theta}(x) & = & \left[\begin{array}{c}
\frac{\exp((\theta_{1}-\theta_{2})^{T}x)}{\exp((\theta_{1}-\theta_{2})^{T}x)+1}\\
\frac{1}{\exp((\theta_{1}-\theta_{2})^{T}x)+1}
\end{array}\right]\\
 & = & \left[\begin{array}{c}
\frac{\exp(\theta'^{T}x)}{\exp(\theta'^{T}x)+1}\\
\frac{1}{\exp(\theta'^{T}x)+1}
\end{array}\right]\\
 & = & \left[\begin{array}{c}
1-\frac{1}{\exp(\theta'^{T}x)+1}\\
\frac{1}{\exp(\theta'^{T}x)+1}
\end{array}\right]
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
We can now observe that the output of the one of the two classes is exactly
 equivalent to the predictions of the logistic regression hypothesis function
\begin_inset Foot
status open

\begin_layout Plain Layout
CS229 Lecture Notes 1 Page 16
\end_layout

\end_inset

: 
\begin_inset Formula $h_{\theta}(x)=g(\theta^{T}x)=\frac{1}{1+\exp(-\theta^{T}x)}$
\end_inset

 when 
\begin_inset Formula $\theta'=\theta$
\end_inset

.
 The other class is obviously 
\begin_inset Formula $1-h_{\theta}(x)$
\end_inset

.
 Therefore, logistic regression is a special case of softmax regression
 where 
\begin_inset Formula $k=2$
\end_inset

.
\end_layout

\begin_layout Section
\start_of_appendix
Gradient Derivations
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
z & = & Wx+b_{1}\\
a & = & f(z)\qquad\mathrm{where}\; f(x)=\tanh(x)\\
h & = & g(U^{T}a+b_{2})\qquad\mathrm{where}\: g(x)=\frac{1}{1+e^{-x}}\\
J(\theta) & = & \frac{1}{m}\sum_{i=1}^{m}\left[-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i0})h_{\theta}(x^{(i)})\right]\:+\;\frac{C}{2m}\left[\sum_{j=1}^{nC}\sum_{k=1}^{H}W_{kj}^{2}+\sum_{k=1}^{H}U_{k}^{2}\right]\\
\frac{\partial J(\theta)}{\partial U} & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}+(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}\right)\frac{\partial h_{\theta}(x^{(i)})}{\partial U}\right]+\frac{C}{2m}(2U)\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}+(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}\right)h_{\theta}(x^{(i)})(1-h_{\theta}(x^{(i)}))\frac{\partial(U^{T}a^{(i)}+b_{2})}{\partial U}\right]+\frac{C}{m}U\\
 &  & \mathrm{since\:}g'(x)=g(x)(1-g(x))\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}+(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}\right)h_{\theta}(x^{(i)})(1-h_{\theta}(x^{(i)}))\, a^{(i)}\right]+\frac{C}{m}U\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\, a^{(i)}\right]+\frac{C}{m}U\\
\frac{\partial J(\theta)}{\partial b_{1}} & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}+(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}\right)\frac{\partial h_{\theta}(x^{(i)})}{\partial U}\right]\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}+(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}\right)h_{\theta}(x^{(i)})(1-h_{\theta}(x^{(i)}))\frac{\partial(U^{T}a^{(i)}+b_{2})}{\partial U}\right]\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}+(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}\right)h_{\theta}(x^{(i)})(1-h_{\theta}(x^{(i)}))\right]\\
\frac{\partial J(\theta)}{\partial W_{kj}} & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\frac{\partial(U^{T}a^{(i)}+b_{2})}{\partial W_{kj}}\right]+\frac{C}{2m}(2W_{kj})\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}\frac{\partial f(z_{k}^{(i)})}{\partial W_{kj}}\right]+\frac{C}{m}W_{kj}\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}f'(z_{k}^{(i)})\frac{\partial(W_{k\cdot}x^{(i)}+(b_{1})_{k})}{\partial W_{kj}}\right]+\frac{C}{m}W_{kj}\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}f'(z_{k}^{(i)})x_{j}^{(i)}\right]+\frac{C}{m}W_{kj}\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}(1-\tanh^{2}z_{k}^{(i)})x_{j}^{(i)}\right]+\frac{C}{m}W_{kj}\\
\mathrm{Let}\:\delta_{k}^{(i)} & = & \left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}(1-\tanh^{2}z_{k}^{(i)})\\
\frac{\partial J(\theta)}{\partial W} & = & \frac{1}{m}\sum_{i=1}^{m}\delta^{(i)}\left(x^{(i)}\right)^{T}+\frac{C}{m}W\\
\frac{\partial J(\theta)}{\partial(b_{1})_{i}} & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\frac{\partial(U^{T}a^{(i)}+b_{2})}{\partial(b_{1})_{i}}\right]\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}\frac{\partial f(z_{k}^{(i)})}{\partial(b_{1})_{i}}\right]\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}f'(z_{k}^{(i)})\frac{\partial(W_{k\cdot}x^{(i)}+(b_{1})_{k})}{\partial W_{kj}}\right]\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)U_{k}f'(z_{k}^{(i)})\right]\\
\frac{\partial J(\theta)}{\partial b_{i}} & = & \frac{1}{m}\sum_{i=1}^{m}\delta^{(i)}\\
\frac{\partial J(\theta)}{\partial L_{k}} & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\frac{\partial(U^{T}a^{(i)}+b_{2})}{\partial L_{k}}\right]\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\left(\sum_{j=1}^{H}U_{j}\frac{\partial f(z_{j}^{(i)})}{\partial L_{k}}\right)\right]\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\left(\sum_{j=1}^{H}U_{j}f'(z_{j}^{(i)})\frac{\partial(W_{j\cdot}x^{(i)}+(b_{1})_{j})}{\partial L_{k}}\right)\right]\\
 & = & \frac{1}{m}\sum_{i=1}^{m}\left[\left(-y^{(i)}(1-h_{\theta}(x^{(i)}))+(1-y^{(i)})h_{\theta}(x^{(i)})\right)\left(\sum_{j=1}^{H}U_{j}f'(z_{j}^{(i)})W_{jk}\right)\right]\\
\frac{\partial J(\theta)}{\partial L} & = & \frac{1}{m}\sum_{i=1}^{m}W^{T}\delta^{(i)}
\end{eqnarray*}

\end_inset


\end_layout

\end_body
\end_document
